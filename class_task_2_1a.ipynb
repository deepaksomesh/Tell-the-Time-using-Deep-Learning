{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classification Model",
   "id": "173ada7e27cfb16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, BatchNormalization,\n",
    "                                     Dropout, Flatten, Dense, Input)\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "6a0c9333677910ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initializing the required parameters (Episodes, Mini-batching, Learning rate)",
   "id": "f0bb19b9538638a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_classes_list = [12, 24, 720]   # 12 (1-hr bins), 24 (30-min bins), 720 (1-min bins)\n",
    "epochs = 60\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4"
   ],
   "id": "efe593fdb1b9e72e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading and processing the data (Converting labels to total minutes, Normalizing pixel values)\n",
    "(We didn't use zipfile to load the data, so mmake sure the .npy files are present in the same directory as notebook)"
   ],
   "id": "368db631219b7a9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Loading data...\")\n",
    "images = np.load(\"images.npy\")\n",
    "labels = np.load(\"labels.npy\")   # shape (N, 2): [hour, minute]\n",
    "\n",
    "# Convert labels to total minutes [0,720)\n",
    "hours = labels[:, 0] % 12\n",
    "minutes = labels[:, 1]\n",
    "total_minutes = (hours * 60 + minutes).astype(int)\n",
    "\n",
    "# Normalize pixel values\n",
    "images = images.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add channel dimension if missing\n",
    "if len(images.shape) == 3:\n",
    "    images = images[..., np.newaxis]  # (N, H, W, 1)"
   ],
   "id": "6199bd026952811e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train/Validation/Test Split of the data (80/10/10)",
   "id": "8744bdf31cc04a06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    images, total_minutes, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ],
   "id": "d8741db349364998"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Converting the total minutes to class index given number of classes and the bins are equal intervals around the clock (cyclic).",
   "id": "19ae404bbf365b54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def minutes_to_class(minutes, n_classes):\n",
    "    bin_size = 720 / n_classes\n",
    "    classes = np.floor(minutes / bin_size).astype(int)\n",
    "    classes = np.clip(classes, 0, n_classes - 1)\n",
    "    return classes"
   ],
   "id": "8e2d596839b5da56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Mapping class index back to central minute of its interval.",
   "id": "8c6e18900df2635e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:43:55.675736Z",
     "start_time": "2025-11-06T12:43:55.671748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def class_to_center_minute(class_idx, n_classes):\n",
    "    bin_size = 720 / n_classes\n",
    "    return ((class_idx + 0.5) * bin_size) % 720"
   ],
   "id": "58aaba38ae2bbfb0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Common Sense Error - Mean circular difference (minimizing around the clock).",
   "id": "d212043654acffc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def common_sense_error(y_true_minutes, y_pred_minutes):\n",
    "    diff = np.abs(y_true_minutes - y_pred_minutes) % 720\n",
    "    diff = np.minimum(diff, 720 - diff)\n",
    "    return np.mean(diff)"
   ],
   "id": "f6d5aca6878ea6a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualizing the loss and accuracy",
   "id": "34fbcb2a34bd6efa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_training(h, n_classes):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(h.history[\"loss\"], label=\"train\")\n",
    "    plt.plot(h.history[\"val_loss\"], label=\"val\")\n",
    "    plt.title(f\"Loss ({n_classes} classes)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(h.history[\"accuracy\"], label=\"train\")\n",
    "    plt.plot(h.history[\"val_accuracy\"], label=\"val\")\n",
    "    plt.title(f\"Accuracy ({n_classes} classes)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "7b8c75d8589ef25f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### A simple CNN architecture with batch normalization to train the model",
   "id": "3829613259b66406"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_base_cnn(input_shape, n_classes):\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inp)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    out = Dense(n_classes, activation=\"softmax\", name=\"classification\")(x)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ],
   "id": "9e7416a2b37a3ec7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training and Results using test set, the plots and CSE",
   "id": "4be937b1c2844fb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = {}\n",
    "\n",
    "for n_classes in n_classes_list:\n",
    "    print(f\"\\nTraining for {n_classes} classes\\n\")\n",
    "\n",
    "    # Convert to discrete class indices\n",
    "    y_train_cls = minutes_to_class(y_train, n_classes)\n",
    "    y_val_cls = minutes_to_class(y_val, n_classes)\n",
    "    y_test_cls = minutes_to_class(y_test, n_classes)\n",
    "\n",
    "    # Build and train CNN\n",
    "    model = build_base_cnn(X_train.shape[1:], n_classes)\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train_cls,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val_cls),\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test_cls, verbose=0)\n",
    "    preds = np.argmax(model.predict(X_test), axis=1)\n",
    "    pred_minutes = class_to_center_minute(preds, n_classes)\n",
    "    mean_err = common_sense_error(y_test, pred_minutes)\n",
    "\n",
    "    print(f\"\\nAccuracy ({n_classes} classes): {test_acc:.4f}\")\n",
    "    print(f\"Mean common-sense error: {mean_err:.2f} minutes\")\n",
    "\n",
    "    results[n_classes] = {\n",
    "        \"accuracy\": float(test_acc),\n",
    "        \"common_sense_error_min\": float(mean_err)\n",
    "    }\n",
    "\n",
    "    plot_training(history, n_classes)\n",
    "\n",
    "for n, res in results.items():\n",
    "    print(f\"{n:>4} classes â†’ acc: {res['accuracy']:.3f}, \"\n",
    "          f\"common-sense err: {res['common_sense_error_min']:.2f} min\")"
   ],
   "id": "32e847952823750d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
